---
title: "XGBoost propensity score estimate"
author: "Matthew Townley"
date: "2/11/2019"
output: html_document
---

```{r setup, echo = F, include = T, eval = T}
a_frame = read.csv(file = "y1314_clean.csv", as.is = T)

libs = c("xgboost", "caret", "rsample", "pdp", "lime", "Matching", "MatchIt")
# lapply(libs, install.packages, character.only = T)
lapply(libs, library, character.only = T)

```


# Intro

Our project started by asking how well chronic absenteeism predicts poor graduation outcomes in high schools in the United States. 

This analysis shifts the question to a causal one, "will reducing absenteeism alone improve graduation outcomes?"

Moving from the first to the second requires not only accounting for the non-random 'assignment' 

In previous analyses




# Approach


1. dichotomize high absenteeism
2. assess standardized differences in covariates
3. then throw them all in the model anyway
4. assess balance

# Dichotomize

First make sure we have rates only $0 <= r <=1$

```{r}
# require('rpart')
a_frame = a_frame[which(!is.na(a_frame$abs_r)),]
a_frame = a_frame[which(a_frame$abs_r <= 1 & a_frame$abs_r > 0),]
a_frame$abs_r = round(a_frame$abs_r, 2)

a_frame = a_frame[which(!is.na(a_frame$grad_r)),]
a_frame = a_frame[which(a_frame$grad_r <= 1 & a_frame$grad_r > 0),]
a_frame$grad_r = round(a_frame$grad_r, 2)
```

```{r linear_combinations, echo = T, include = T, eval = T}
# really need to clean this up


# too soon, but want to get this code in here
# from page 119 of faraway
# to check for linear combinations
x = as.matrix(a_frame[,-1])
e = eigen(t(x) %*% x)
e$val
sqrt(e$val[1]/e$val)
car::vif(lm(grad_r ~ ., data = a_frame))

var.x = var(x)
var.x.eigen = eigen(var.x)


data.frame(variable = colnames(x), eigen = e$values, var_eigen = var.x.eigen$values, vif = car::vif(lm(grad_r ~ ., data = a_frame)))

# *not* a lot of collinearity with absenteeism (!)
summary(lm(abs_r ~ ., data = data.frame(x)))

# ... but this one
summary(lm(tot_apexam_oneormore ~ ., data = data.frame(x)))
# why?
colnames(x)
e$vectors[,32]
# look for very similar values
e$vectors[,32] %>% signif(3)

# distance between each eigenvalue in the eigenvector for the highest VIF 
edist = dist(e$vectors[,32], method = "euclidean", diag = T)
# look at just the values for variable # 32 (tot_apexam_oneormore)
as.matrix(edist)[32,] %>% signif(3)
# ...maybe distances as a proportion of the max?
as.matrix(edist)[32,] / max(as.matrix(edist)[32,])
# closest are
sort(as.matrix(edist)[32,] / max(as.matrix(edist)[32,]) )
# variable names for the 8 closest
closest.idx = sort(as.matrix(edist)[32,] / max(as.matrix(edist)[32,]) ) %>% names %>% head(8) %>% as.numeric
colnames(x)[closest.idx]
# no surprise, can tot_apexam_oneormore do the work of the rest of the variables?

# is the VIF the diag of the cov matrix?
(cov(x) %>% diag)^(1/2) 
car::vif(lm(grad_r ~ ., data = a_frame)) # not even close

# what happens if we drop tot_apexam_oneormore?
car::vif(lm(grad_r ~ . - tot_apexam_oneormore, data = a_frame))

# Or drop all the ap exam variables except oneormore?
car::vif(lm(grad_r ~ ., data = a_frame[,-c(29:34,36)]))

# now how about the math variables
data.frame(colnames(x), e$values)
e$vectors[14,]
as.matrix(edist)[14,] / max(as.matrix(edist)[14,])
closest.idx = sort(as.matrix(edist)[14,] / max(as.matrix(edist)[14,]) ) %>% names %>% head(8) %>% as.numeric
colnames(x)[closest.idx]
car::vif(lm(grad_r ~ . -tot_algenr_gs0708, data = a_frame[,-c(29:34,36)]))

# looks like dropping all the ap variables + early algebra enrollment
# removes most of the problematic collinearities
keepvars = names(a_frame[,-c(15,29:34,36)])
car::vif(lm(grad_r ~ ., data = a_frame[,keepvars]))

```

Estimate quick linear and log linear models
```{r log_linear_model}
lmod = lm(grad_r ~ ., data = a_frame[,keepvars])
llmod = lm(I(log(grad_r)) ~ ., data = a_frame[,keepvars])

summary(llmod)
exp(coef(llmod))

```

## Quick exploratory

Take a look at the distribution of abseteeism and graduation rates

Absenteeism:

```{r}

quantile(a_frame$abs_r, probs = 0:10 / 10)
quantile(a_frame$abs_r, probs = 0:4 / 4)
boxplot(a_frame$abs_r)
plot(density(a_frame$abs_r))

# d.mod = rpart(grad_r ~ abs_r, data = a_frame, method = 'anova')
# summary(d.mod)
# plot(d.mod)
# text(d.mod)
```

Graduation:

```{r}

quantile(a_frame$grad_r, probs = 0:10 / 10)
quantile(a_frame$grad_r, probs = 0:4 / 4)
boxplot(a_frame$grad_r)
plot(density(a_frame$grad_r))

```


Means within decile strata (good example of split/apply/combine)

```{r}
quantile(a_frame$abs_r, probs = 0:4 / 4)
quantile(a_frame$abs_r, probs = 0:10 / 10)
f_absr = cut(a_frame$abs_r, breaks = quantile(a_frame$abs_r, probs = 0:10 / 10))
# data.frame(a_frame$abs_r, f_absr) %>% head(100)

s_gradr = split(a_frame$grad_r, f_absr)
lapply(s_gradr, mean, na.rm = T)

```

```{r grad_r_split_exp}
ta = a_frame[,c("abs_r", "grad_r")]

splitpoint = quantile(ta$abs_r, probs = 0.9)
ta$treat = 0
ta[which(ta$abs_r > splitpoint), "treat"] = 1

with(ta, boxplot(grad_r ~ treat))

```

## Where to split

Find the split in absenteeism rates that gives the most information about the relationship with graduation rates.

Is that cheating? Not if we think there's a crucial threshold of absenteeism rates at which graduation outcomes are really, really poor.

Use entropy, gini, RMSE.

I'm kinda proud of this

put the formulas in (later) from here: http://www.stats.ox.ac.uk/~flaxman/HT17_lecture13.pdf

RMSE

$rmse = \sqrt{\frac{\sum_i{(\hat{y}-y_i)^2}}{n}}$

Entropy [Shannon](https://en.wikipedia.org/wiki/Entropy_(information_theory))

$H(X) = \sum_{k=1}^K{P(X = a_k) \times -lnP(X = a_k)}$

Gini

$G(X) = \sum_{k=1}^K{2P(X = a_k)(1 - P(X = a_k))}$


Reading back, I need to use Information Gain to split, then calculate entropies, etc...

```{r define_functions}
rmse = function(vec) {
  vec = vec[which(!is.na(vec))]
  sqrt( sum((vec - mean(vec))**2) / length(vec) )
}

entropy = function(vec) {
  pr = table(vec) / sum(table(vec))
  sum(pr * -log(pr))
}

gini = function(vec) {
  pr = table(vec) / sum(table(vec))
  sum(pr * (1-pr) * 2) 
}

```

Take a look at every possible split

```{r dichotomize_abs_exp1}

gradr = a_frame[,"grad_r"]
absr = a_frame[,"abs_r"]

splitpoint = 0.9
f_absr = cut(absr, breaks = quantile(absr, probs = c(0,splitpoint,1)))

s_gradr = split(gradr, f = f_absr)
sapply(s_gradr, rmse)
sapply(s_gradr, gini)
sapply(s_gradr, entropy)


```

```{r dichotomize_abs_exp_all}

splitpoints = c(0.2, 0.5, 0.8)

split_walker = function(y, f, splits, FN = rmse) {
  
  FUN = match.fun(FN)
  
  f_f = cut(f, breaks = c(0,splits,1)) # if you want quantiles, define outside the function
  s_y = split(y, f = f_f)
  sapply(s_y, FUN)
}

splitpoints = 1:99 / 100

entropies = lapply(splitpoints, split_walker, y = gradr, f = absr, FN = entropy) 
entropies = do.call('rbind', entropies)
rownames(entropies) = splitpoints
entropies[order(entropies[,2], decreasing = F),][1:10,]


ginis = lapply(splitpoints, split_walker, y = gradr, f = absr, FN = gini) 
ginis = do.call('rbind', ginis)
rownames(ginis) = splitpoints

mses = lapply(splitpoints, split_walker, y = gradr, f = absr, FN = rmse) 
mses = do.call('rbind', mses)
rownames(mses) = splitpoints
mses[order(mses[,2], decreasing = T),][1:10,]


```




```{r dichotomize_abs}
f_absr = cut(a_frame$abs_r, breaks = quantile(a_frame$abs_r, probs = c(0,0.85,1)))
lapply(split(a_frame$grad_r, f_absr), entropy)
lapply(split(a_frame$grad_r, f_absr), function(x) {mean(x, na.rm = T)})

a_frame$treat = 0
a_frame[which(a_frame$abs_r >= quantile(a_frame$abs_r, 0.85)),"treat"] = 1

aggregate(grad_r ~ treat, data = a_frame, FUN = "mean")

```

# standardized differences

```{r standardized_diff}

a_split = split(a_frame, f = a_frame$treat)

means = aggregate(x = a_frame, by = list(a_frame$treat), function(x) { mean(x, na.rm = T)})
sds = aggregate(x = a_frame, by = list(a_frame$treat), function(x) { sd(x, na.rm = T)})

d = (means[2,] - means[1,]) / sqrt(apply(sds**2, 2, sum) / 2)
sort(d)
```


# calculate propensity score


```{r gbm_setup, echo = F, include = T, eval = T}
libs = c("xgboost", "caret", "rsample", "pdp", "lime")
# lapply(libs, install.packages, character.only = T)
lapply(libs, library, character.only = T)

a_frame = a_frame[which(!is.na(a_frame$grad_r)),]
a_frame = a_frame[which(a_frame$abs_r < 1),]

summary(a_frame$abs_r)

# set.seed(1274)
# g_split <- initial_split(a_frame, prop = (3/4))
# g_train <- training(g_split)
# g_test <- testing(g_split)
# 
# 
# features_train = g_train[,-1]
# response_train = g_train[,"grad_r"]
# 
# features_test = g_test[,-1]
# response_test = g_test[,"grad_r"]

features_train = a_frame[,-c(1,2,38)]
response_train = a_frame[,'treat']

```

```{r logr_prop_score, echo = T, include = T, eval = T}

pp_fit = glm(treat ~ ., data = a_frame[, -c(1,2)], family = binomial())
logit = fitted(pp_fit)

odds = exp(logit)
prob = odds / (1 + odds)

# a_frame$pihat.log = pp_fit$fitted.values
```




```{r}
#Create Matched Dataset
t_frame = data.frame(a_frame, pps = fitted(pp_fit), stringsAsFactors = F)

# lm(treat ~ ., data = a_frame[, -c(1,2)])
# to make optimach work
options("optmatch_max_problem_size" = Inf)
opt.mtch <- matchit(reformulate(colnames(t_frame[,-36]), response = 'treat'), 
                    data = t_frame,
                     distance = t_frame[,"pps"],
                     method = "optimal")
matched.opt <- match.data(opt.mtch)

plot(density(matched.opt[which(matched.opt$treat == 0), "pps"]))
points(density(matched.opt[which(matched.opt$treat == 1), "pps"]), type = "l", lty = 2)
```

Net of confounding, there appears to be about a 10% decrease in graduation rates attributable to high absenteeism alone.

```{r}
#Outcome Analysis
mod.opt.mtch <- lm(grad_r ~ treat, data=matched.opt)
summary(mod.opt.mtch) # this is cool


```

```{r}
a_split = split(matched.opt, f = matched.opt$treat)

means = aggregate(x = matched.opt, by = list(matched.opt$treat), function(x) { mean(x, na.rm = T)})
sds = aggregate(x = matched.opt, by = list(matched.opt$treat), function(x) { sd(x, na.rm = T)})

d = (means[2,] - means[1,]) / sqrt(apply(sds**2, 2, sum) / 2)
sort(d)

```



```{r gbm_prop_score, echo = T, include = T, eval = T}
set.seed(1228)

params = list(
  eta = .1,
  max_depth = 9,
  min_child_weight = 3,
  subsample = .8,
  colsample_bytree = .9
)

start.time = proc.time()

xgb.fit1 = xgboost(
  data = features_train %>% as.matrix,
  label = response_train,
  params = params,
  missing = NA,
  nrounds = 1000,
  nfold = 5,
  # objective = "reg:logistic",  # for regression models
  verbose = 0,               # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees

  )
proc.time() - start.time

quantile(response_train, probs = 0:4 / 4) %>% round(3)
xgb.fit1$evaluation_log[which.min(xgb.fit1$evaluation_log$train_rmse),]

logit = predict(xgb.fit1, features_train %>% as.matrix)

odds = exp(logit)
prob = odds / (1 + odds)
# summary(xgb.fit1)
```

```{r, echo = T, eval = F, include = F}
source('~/Documents/rfuns/mMisc/R/mMisc.R')

roc_ps = roclines(features_train, logit)
plot(roc_ps, type = "l")
abline(a = 0, 1)

# points(x = (0:100 / 100), y = (0:100 / 100), lty = 2, col = "grey40", type = 'l')
# plot(x = (0:100 / 100), y = (0:100 / 100), lty = 2, col = "grey40", type = 'l', add = T)
```

# check overlap

```{r}
boxplot(split(prob, f = a_frame$treat), xlab = "treatment", ylab = "estimated propensity scores")

# plot(density(prob))
```

```{r}

ta = data.frame(pp = prob, treat = a_frame$treat)
plot(density(ta[which(ta$treat == 0), "pp"]), col = "blue")
points(density(ta[which(ta$treat == 1), "pp"]), type = "l", col = "green")
```



